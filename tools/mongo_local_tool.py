# tools/mongo_local_tool.py

"""
MongoLocalTool -> HybridLiteratureTool
æœ¬åœ°å‘é‡æ•°æ®åº“ + PubMed åœ¨çº¿æ··åˆæ£€ç´¢å·¥å…· (æ”¯æŒæ‰¹é‡ç²¾å‡†å½’ä½ç‰ˆ)

åŠŸèƒ½ï¼š
1. æ··åˆæ£€ç´¢ï¼šåŒæ—¶ä»æœ¬åœ° MongoDB (Vector) å’Œ PubMed Online è·å–è¯æ®
2. å¤šè·¯å¬å›ï¼šGene æ¨¡å¼ä¸‹è‡ªåŠ¨ç”Ÿæˆå¤šç»´æŸ¥è¯¢ (æœºåˆ¶/é¢„å/æ²»ç–—)
3. æ‰¹é‡å¤„ç†ï¼šæ”¯æŒä¼ å…¥ genes åˆ—è¡¨ï¼Œè‡ªåŠ¨å¾ªç¯å¹¶æ ‡è®°å½’å±ï¼Œå®ç°ç²¾å‡†æº¯æº
"""

import logging
import re
import time
import numpy as np
import faiss
from typing import List, Dict
from itertools import groupby
from pymongo import MongoClient
from sentence_transformers import SentenceTransformer
from tools.pubmed_tool import PubMedTool

logger = logging.getLogger(__name__)

# === å…¨å±€èµ„æº (å•ä¾‹æ¨¡å¼) ===
_GLOBAL_MODEL = None
_GLOBAL_INDEX = None
_GLOBAL_DOC_MAP = []

class MongoLocalTool:
    
    def __init__(self, host: str = "localhost", port: int = 27017, 
                 db_name: str = "bio", collection_name: str = "evidence_chunks"):
        """
        åˆå§‹åŒ–å·¥å…·
        """
        self.host = host
        self.port = port
        self.db_name = db_name
        self.collection_name = collection_name
        self.client = None
        self.db = None
        self.collection = None
        
        # åˆå§‹åŒ– PubMed åœ¨çº¿å·¥å…·
        self.pubmed = PubMedTool()

    def _connect(self):
        """è¿æ¥ MongoDB"""
        if self.client: return
        try:
            self.client = MongoClient(host=self.host, port=self.port, serverSelectionTimeoutMS=2000)
            self.db = self.client[self.db_name]
            self.collection = self.db[self.collection_name]
            # self.client.admin.command('ping') # å¯é€‰ï¼šæ£€æŸ¥è¿æ¥
            logger.debug(f"Connected to MongoDB {self.host}:{self.port}/{self.db_name}")
        except Exception as e:
            logger.exception(f"Failed to connect to MongoDB: {e}")

    def _ensure_resources(self):
        """åŠ è½½æ¨¡å‹ä¸æ„å»ºç´¢å¼•"""
        global _GLOBAL_MODEL, _GLOBAL_INDEX, _GLOBAL_DOC_MAP
        
        self._connect()
        
        if _GLOBAL_MODEL is None:
            try:
                logger.info(">>> [HybridTool] Loading model (all-MiniLM-L6-v2)...")
                _GLOBAL_MODEL = SentenceTransformer('all-MiniLM-L6-v2')
            except Exception as e:
                logger.error(f"Failed to load SentenceTransformer: {e}")

        if _GLOBAL_INDEX is None and self.collection is not None:
            try:
                logger.info(">>> [HybridTool] Building FAISS index from MongoDB...")
                start_time = time.time()
                # åªè¯»å–å¸¦å‘é‡çš„æ•°æ®
                cursor = self.collection.find(
                    {"vector": {"$exists": True}},
                    {"vector": 1, "text": 1, "section": 1, "paper_title": 1, "source_filename": 1}
                )
                
                vectors = []
                doc_map = []
                for doc in cursor:
                    vec = doc.get('vector')
                    if vec and len(vec) > 0:
                        vectors.append(np.array(vec, dtype='float32'))
                        doc_map.append({
                            'id': str(doc.get('_id')),
                            'text': doc.get('text', ''),
                            'section': doc.get('section', 'Unknown'),
                            'paper_title': doc.get('paper_title', 'Unknown'),
                            'source': doc.get('source_filename', 'LocalDB')
                        })
                
                if vectors:
                    vector_matrix = np.array(vectors)
                    dimension = vector_matrix.shape[1]
                    index = faiss.IndexFlatIP(dimension)
                    index.add(vector_matrix)
                    _GLOBAL_INDEX = index
                    _GLOBAL_DOC_MAP = doc_map
                    logger.info(f">>> Index built with {len(vectors)} chunks in {time.time()-start_time:.2f}s")
                else:
                    logger.warning(">>> No vectors found in local database.")
                    _GLOBAL_INDEX = None
                    _GLOBAL_DOC_MAP = []
            except Exception as e:
                logger.warning(f"Failed to build local index: {e}")

    def _calculate_keyword_score(self, query: str, text: str) -> float:
        if not query or not text: return 0.0
        q_terms = set(re.findall(r'\w+', query.lower()))
        t_terms = set(re.findall(r'\w+', text.lower()))
        if not q_terms: return 0.0
        return len(q_terms.intersection(t_terms)) / len(q_terms)

    # === 1. æœ¬åœ°æ£€ç´¢æ ¸å¿ƒ ===
    def _search_local_core(self, query: str, top_k: int = 5) -> List[Dict]:
        self._ensure_resources()
        if _GLOBAL_INDEX is None or not _GLOBAL_DOC_MAP:
            return []

        try:
            query_vector = _GLOBAL_MODEL.encode([query])
            query_vector = np.array(query_vector, dtype='float32')
            D, I = _GLOBAL_INDEX.search(query_vector, min(50, len(_GLOBAL_DOC_MAP)))
            
            candidates = []
            for rank, idx in enumerate(I[0]):
                if idx == -1: continue
                doc_data = _GLOBAL_DOC_MAP[idx]
                vec_score = float(D[0][rank])
                kw_score = self._calculate_keyword_score(query, doc_data['text'])
                
                # æ··åˆæ‰“åˆ†
                hybrid_score = (0.7 * vec_score) + (0.3 * kw_score)
                
                # ç« èŠ‚åŠ æƒ
                section = str(doc_data['section']).lower()
                multiplier = 1.0
                if any(x in section for x in ['result', 'discussion', 'conclusion']):
                    multiplier = 1.2
                elif 'abstract' in section:
                    multiplier = 1.1
                
                candidates.append({
                    "content": doc_data['text'],
                    "source_metadata": {
                        "paper_title": doc_data['paper_title'],
                        "section": doc_data['section'],
                        "filename": doc_data['source']
                    },
                    "scores": {"final": round(hybrid_score * multiplier, 4)},
                    "source_type": "Local" # æ ‡è®°æ¥æº
                })
            
            candidates.sort(key=lambda x: x['scores']['final'], reverse=True)
            return candidates[:top_k]
        except Exception as e:
            logger.error(f"Local search failed: {e}")
            return []

    # === 2. æ··åˆæ‰§è¡Œé€»è¾‘ (Local + PubMed) ===
    def _hybrid_search(self, query: str, top_k_local: int = 5, top_k_online: int = 5) -> List[Dict]:
        """åˆå¹¶æœ¬åœ°å’Œåœ¨çº¿ç»“æœ"""
        # 1. æœ¬åœ°æ£€ç´¢
        local_res = self._search_local_core(query, top_k=top_k_local)
        
        # 2. åœ¨çº¿æ£€ç´¢ (è°ƒç”¨å¤–éƒ¨å·¥å…·)
        online_res = self.pubmed.search(query, max_results=top_k_online)
        
        # 3. åˆå¹¶
        combined = local_res + online_res
        return combined

    def _search_evidence_by_gene(self, gene_name: str) -> List[Dict]:
        """é’ˆå¯¹ Gene çš„å¤šè·¯æ··åˆå¬å›"""
        # é’ˆå¯¹è‚ç™Œ (Hepatocellular Carcinoma) çš„ç‰¹å®šæŸ¥è¯¢æ¨¡æ¿
        # ä¹Ÿå¯ä»¥ä» context é‡Œä¼  disease è¿›æ¥åŠ¨æ€æ‹¼æ¥
        queries = [
            ("clinical", f"{gene_name} hepatocellular carcinoma prognosis survival"),
            ("mechanism", f"{gene_name} signaling pathway liver cancer mechanism"),
            ("therapy", f"{gene_name} inhibitor therapeutic target HCC")
        ]
        
        all_results = []
        seen_hashes = set()
        
        # æ¯ä¸ªæ–¹é¢åªå–æœ€ç²¾åçš„ (æœ¬åœ°2 + åœ¨çº¿1)ï¼Œé¿å…ç»“æœçˆ†ç‚¸
        for aspect, query_text in queries:
            results = self._hybrid_search(query_text, top_k_local=2, top_k_online=1)
            
            for item in results:
                content_hash = hash(item['content'][:100])
                if content_hash not in seen_hashes:
                    item['aspect'] = aspect
                    item['matched_query'] = query_text
                    # ã€å…³é”®ã€‘ä¸è¦åœ¨è¿™é‡ŒåŠ  related_geneï¼Œè€Œåœ¨å¤–å±‚åŠ ï¼Œé˜²æ­¢å¤ç”¨é€»è¾‘æ··ä¹±
                    all_results.append(item)
                    seen_hashes.add(content_hash)
        
        return all_results

    def _generate_summary(self, results: List[Dict], subject: str, mode: str) -> str:
        """ç”Ÿæˆ Markdown ç»¼è¿° (æ”¯æŒå¤šåŸºå› åˆ†ç»„)"""
        if not results:
            return f"æœªæ‰¾åˆ°å…³äº {subject} çš„æ–‡çŒ®è¯æ® (æœ¬åœ°+åœ¨çº¿)ã€‚"
            
        lines = []
        lines.append(f"### ğŸ“š æ–‡çŒ®æ£€ç´¢ç»¼è¿°: {subject}")
        lines.append(f"> æ€»è®¡æ¡ç›®: {len(results)} \n")
        
        # å¦‚æœæ˜¯æ‰¹é‡æ¨¡å¼ï¼ŒæŒ‰ related_gene åˆ†ç»„å±•ç¤º
        if mode == "batch_gene":
            # å…ˆæŒ‰ gene æ’åºï¼Œå† groupby
            results.sort(key=lambda x: x.get('related_gene', 'Unknown'))
            for gene, gene_items in groupby(results, key=lambda x: x.get('related_gene', 'Unknown')):
                lines.append(f"#### ğŸ§¬ åŸºå› : {gene}")
                gene_items_list = list(gene_items)
                # å†…éƒ¨å†æŒ‰ aspect åˆ†ç»„
                gene_items_list.sort(key=lambda x: x.get('aspect', 'general'))
                for aspect, group in groupby(gene_items_list, key=lambda x: x.get('aspect', 'general')):
                    aspect_icon = {"clinical": "ğŸ¥", "mechanism": "ğŸ”¬", "therapy": "ğŸ’Š", "general": "ğŸ”"}.get(aspect, "ğŸ“„")
                    lines.append(f"**{aspect_icon} {aspect.capitalize()}**")
                    for item in group:
                        content = item['content'].replace('\n', ' ')[:200] + "..."
                        src = item.get('source_type', 'Unknown')
                        title = item['source_metadata']['paper_title']
                        lines.append(f"- [{src}] {content} *({title})*")
                lines.append("")
        else:
            # å•åŸºå› æˆ– Query æ¨¡å¼
            results.sort(key=lambda x: x.get('aspect', 'general'))
            for aspect, group in groupby(results, key=lambda x: x.get('aspect', 'general')):
                lines.append(f"**{aspect.capitalize()}**")
                for item in group:
                    content = item['content'].replace('\n', ' ')[:250] + "..."
                    lines.append(f"- {content}")
            lines.append("")
        
        return "\n".join(lines)

    def run(self, context: Dict) -> Dict:
        """
        å·¥å…·å…¥å£ - æ”¯æŒæ‰¹é‡ genes å¤„ç†
        """
        print(f"[MongoLocalTool]: æ­£åœ¨æ£€ç´¢æ–‡çŒ®...")
        
        gene = context.get("gene")
        genes = context.get("genes") # è·å–åˆ—è¡¨å‚æ•°
        query = context.get("query")
        
        results = []
        search_subject = ""
        search_mode = ""

        try:
            # === ä¼˜å…ˆå¤„ç†æ‰¹é‡åŸºå› åˆ—è¡¨ ===
            if genes and isinstance(genes, list) and len(genes) > 0:
                search_mode = "batch_gene"
                search_subject = f"Batch of {len(genes)} genes"
                # é™åˆ¶æ‰¹é‡å¤„ç†æ•°é‡ï¼Œé˜²æ­¢è¶…æ—¶ (ä¾‹å¦‚åªæŸ¥å‰ 10 ä¸ªï¼Œæˆ–å…¨éƒ¨)
                # target_genes = genes[:10] 
                target_genes = genes # å…¨é‡æŸ¥è¯¢ï¼ŒPlannerä¼šæ§åˆ¶ä¼ å…¥æ•°é‡
                
                print(f"  > æ‰¹é‡æ£€ç´¢æ¨¡å¼: {len(target_genes)} ä¸ªåŸºå› ")
                
                for g in target_genes:
                    if not g: continue
                    # æ£€ç´¢å•åŸºå› 
                    g_res = self._search_evidence_by_gene(g)
                    
                    # ã€æ ¸å¿ƒä¿®æ”¹ã€‘ç²¾å‡†æ ‡è®°ï¼šä¸ºæ¯æ¡ç»“æœæ‰“ä¸Š related_gene æ ‡ç­¾
                    for item in g_res:
                        item['related_gene'] = g
                    
                    results.extend(g_res)
                    print(f"    - {g}: æ‰¾åˆ° {len(g_res)} æ¡è¯æ®")

            # === å¤„ç†å•ä¸ªåŸºå›  ===
            elif gene:
                search_subject = gene
                search_mode = "single_gene"
                logger.info(f"Running Gene Mode for: {gene}")
                results = self._search_evidence_by_gene(gene)
                for r in results: r['related_gene'] = gene # ä¿æŒä¸€è‡´æ€§
                
            # === å¤„ç†é€šç”¨æ–‡æœ¬æŸ¥è¯¢ ===
            elif query:
                search_subject = query
                search_mode = "general_query"
                logger.info(f"Running General Mode for: {query}")
                results = self._hybrid_search(query, top_k_local=3, top_k_online=3)
                for r in results: r['aspect'] = 'general'
            
            else:
                return {"type": "search_literature", "error": "No gene/genes/query provided"}

            # ç”Ÿæˆç»¼è¿° (ä¾› LLM é˜…è¯»)
            summary = self._generate_summary(results, search_subject, search_mode)

            # è¿”å›ç»“æ„ (raw_results ä¾› Planner ç²¾å‡†æå–)
            return {
                "type": "search_literature",
                "subject": search_subject,
                "search_mode": search_mode,
                "n_results": len(results),
                "summary": summary,
                "raw_results": results, # è¿™é‡Œçš„æ¯ä¸ª item éƒ½å¿…é¡»åŒ…å« 'related_gene'
                "error": None
            }

        except Exception as e:
            logger.exception(f"Error in Hybrid Tool: {e}")
            return {
                "type": "search_literature",
                "error": str(e),
                "summary": f"æ£€ç´¢å‡ºé”™: {str(e)}",
                "results": []
            }

# === éªŒè¯ ===
if __name__ == "__main__":
    import sys
    logging.basicConfig(level=logging.INFO)
    print("ğŸš€ Testing Hybrid Tool (Batch Mode)...")
    
    tool = MongoLocalTool(db_name="bio", collection_name="evidence_chunks")
    # æ¨¡æ‹Ÿæ‰¹é‡æŸ¥è¯¢
    res = tool.run({"genes": ["TP53", "MAGEA1", "UNKNOWN_GENE_123"]})
    print(f"\n{res['summary']}")
    
    # éªŒè¯ raw_results ç»“æ„
    print("\n[Check Raw Results]:")
    for r in res['raw_results'][:3]:
        print(f"Gene: {r.get('related_gene')} | Title: {r['source_metadata']['paper_title']}")