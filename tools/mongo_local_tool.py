# tools/mongo_local_tool.py

"""
MongoLocalTool -> HybridLiteratureTool
æœ¬åœ°å‘é‡æ•°æ®åº“ + PubMed åœ¨çº¿æ··åˆæ£€ç´¢å·¥å…· (æ”¯æŒæ‰¹é‡ç²¾å‡†å½’ä½ç‰ˆ)

åŠŸèƒ½ï¼š
1. æ··åˆæ£€ç´¢ï¼šåŒæ—¶ä»æœ¬åœ° MongoDB (Vector) å’Œ PubMed Online è·å–è¯æ®
2. å¤šè·¯å¬å›ï¼šGene æ¨¡å¼ä¸‹è‡ªåŠ¨ç”Ÿæˆå¤šç»´æŸ¥è¯¢ (æœºåˆ¶/é¢„å/æ²»ç–—)
3. æ‰¹é‡å¤„ç†ï¼šæ”¯æŒä¼ å…¥ genes åˆ—è¡¨ï¼Œè‡ªåŠ¨å¾ªç¯å¹¶æ ‡è®°å½’å±ï¼Œå®ç°ç²¾å‡†æº¯æº
"""

import logging, re, time, faiss, json
import numpy as np
from typing import List, Dict
from itertools import groupby
from pymongo import MongoClient
from datetime import datetime
from sentence_transformers import SentenceTransformer
from tools.pubmed_tool import PubMedTool
from tools.summary_tool import summary

logger = logging.getLogger(__name__)

# === å…¨å±€èµ„æº (å•ä¾‹æ¨¡å¼) ===
_GLOBAL_MODEL = None
_GLOBAL_INDEX = None
_GLOBAL_DOC_MAP = []

class MongoLocalTool:
    
    def __init__(self, host: str = "localhost", port: int = 27017, 
                 db_name: str = "bio", collection_name: str = "evidence_chunks"):
        """
        åˆå§‹åŒ–å·¥å…·
        """
        self.host = host
        self.port = port
        self.db_name = db_name
        self.collection_name = collection_name
        self.client = None
        self.db = None
        self.collection = None
        
        # åˆå§‹åŒ– PubMed åœ¨çº¿å·¥å…·
        self.pubmed = PubMedTool()

    def _connect(self):
        """è¿æ¥ MongoDB"""
        if self.client: return
        try:
            self.client = MongoClient(host=self.host, port=self.port, serverSelectionTimeoutMS=2000)
            self.db = self.client[self.db_name]
            self.collection = self.db[self.collection_name]
            # self.client.admin.command('ping') # å¯é€‰ï¼šæ£€æŸ¥è¿æ¥
            logger.debug(f"Connected to MongoDB {self.host}:{self.port}/{self.db_name}")
        except Exception as e:
            logger.exception(f"Failed to connect to MongoDB: {e}")

    def _ensure_resources(self):
        """åŠ è½½æ¨¡å‹ä¸æ„å»ºç´¢å¼•"""
        global _GLOBAL_MODEL, _GLOBAL_INDEX, _GLOBAL_DOC_MAP
        
        self._connect()
        
        if _GLOBAL_MODEL is None:
            try:
                logger.info(">>> [HybridTool] Loading model (all-MiniLM-L6-v2)...")
                _GLOBAL_MODEL = SentenceTransformer('all-MiniLM-L6-v2')
            except Exception as e:
                logger.error(f"Failed to load SentenceTransformer: {e}")

        if _GLOBAL_INDEX is None and self.collection is not None:
            try:
                logger.info(">>> [HybridTool] Building FAISS index from MongoDB...")
                start_time = time.time()
                # åªè¯»å–å¸¦å‘é‡çš„æ•°æ®
                cursor = self.collection.find(
                    {"vector": {"$exists": True}},
                    {"vector": 1, "text": 1, "section": 1, "paper_title": 1, "source_filename": 1}
                )
                
                vectors = []
                doc_map = []
                for doc in cursor:
                    vec = doc.get('vector')
                    if vec and len(vec) > 0:
                        vectors.append(np.array(vec, dtype='float32'))
                        doc_map.append({
                            'id': str(doc.get('_id')),
                            'text': doc.get('text', ''),
                            'section': doc.get('section', 'Unknown'),
                            'paper_title': doc.get('paper_title', 'Unknown'),
                            'source': doc.get('source_filename', 'LocalDB')
                        })
                
                if vectors:
                    vector_matrix = np.array(vectors)
                    dimension = vector_matrix.shape[1]
                    index = faiss.IndexFlatIP(dimension)
                    index.add(vector_matrix)
                    _GLOBAL_INDEX = index
                    _GLOBAL_DOC_MAP = doc_map
                    logger.info(f">>> Index built with {len(vectors)} chunks in {time.time()-start_time:.2f}s")
                else:
                    logger.warning(">>> No vectors found in local database.")
                    _GLOBAL_INDEX = None
                    _GLOBAL_DOC_MAP = []
            except Exception as e:
                logger.warning(f"Failed to build local index: {e}")

    def _calculate_keyword_score(self, query: str, text: str) -> float:
        if not query or not text: return 0.0
        q_terms = set(re.findall(r'\w+', query.lower()))
        t_terms = set(re.findall(r'\w+', text.lower()))
        if not q_terms: return 0.0
        return len(q_terms.intersection(t_terms)) / len(q_terms)

    # === 1. æœ¬åœ°æ£€ç´¢æ ¸å¿ƒ ===
    def _search_local_core(self, query: str, top_k: int = 5) -> List[Dict]:
        self._ensure_resources()
        if _GLOBAL_INDEX is None or not _GLOBAL_DOC_MAP:
            return []

        try:
            query_vector = _GLOBAL_MODEL.encode([query])
            query_vector = np.array(query_vector, dtype='float32')
            D, I = _GLOBAL_INDEX.search(query_vector, min(50, len(_GLOBAL_DOC_MAP)))
            
            candidates = []
            for rank, idx in enumerate(I[0]):
                if idx == -1: continue
                doc_data = _GLOBAL_DOC_MAP[idx]
                vec_score = float(D[0][rank])
                kw_score = self._calculate_keyword_score(query, doc_data['text'])
                
                # æ··åˆæ‰“åˆ†
                hybrid_score = (0.7 * vec_score) + (0.3 * kw_score)
                
                # ç« èŠ‚åŠ æƒ
                section = str(doc_data['section']).lower()
                multiplier = 1.0
                if any(x in section for x in ['result', 'discussion', 'conclusion']):
                    multiplier = 1.2
                elif 'abstract' in section:
                    multiplier = 1.1
                
                candidates.append({
                    "content": doc_data['text'],
                    "source_metadata": {
                        "paper_title": doc_data['paper_title'],
                        "section": doc_data['section'],
                        "filename": doc_data['source']
                    },
                    "scores": {"final": round(hybrid_score * multiplier, 4)},
                    "source_type": "Local" # æ ‡è®°æ¥æº
                })
            
            candidates.sort(key=lambda x: x['scores']['final'], reverse=True)
            return candidates[:top_k]
        except Exception as e:
            logger.error(f"Local search failed: {e}")
            return []

    # === 2. æ··åˆæ‰§è¡Œé€»è¾‘ (Local + PubMed) ===
    def _hybrid_search(self, query: str, top_k_local: int = 5, top_k_online: int = 5) -> List[Dict]:
        """åˆå¹¶æœ¬åœ°å’Œåœ¨çº¿ç»“æœ"""
        # 1. æœ¬åœ°æ£€ç´¢
        local_res = self._search_local_core(query, top_k=top_k_local)
        
        # 2. åœ¨çº¿æ£€ç´¢ (è°ƒç”¨å¤–éƒ¨å·¥å…·)
        online_res = self.pubmed.search(query, max_results=top_k_online)
        
        # 3. åˆå¹¶
        combined = local_res + online_res
        return combined

    def _search_evidence_by_gene(self, gene_name: str) -> List[Dict]:
        """é’ˆå¯¹ Gene çš„å¤šè·¯æ··åˆå¬å›"""
        # é’ˆå¯¹è‚ç™Œ (Hepatocellular Carcinoma) çš„ç‰¹å®šæŸ¥è¯¢æ¨¡æ¿
        # ä¹Ÿå¯ä»¥ä» context é‡Œä¼  disease è¿›æ¥åŠ¨æ€æ‹¼æ¥
        queries = [
            ("clinical", f"{gene_name} hepatocellular carcinoma prognosis survival"),
            ("mechanism", f"{gene_name} signaling pathway liver cancer mechanism"),
            ("therapy", f"{gene_name} inhibitor therapeutic target HCC")
        ]
        
        all_results = []
        seen_hashes = set()
        
        # æ¯ä¸ªæ–¹é¢åªå–æœ€ç²¾åçš„ (æœ¬åœ°2 + åœ¨çº¿1)ï¼Œé¿å…ç»“æœçˆ†ç‚¸
        for aspect, query_text in queries:
            results = self._hybrid_search(query_text, top_k_local=2, top_k_online=1)
            
            for item in results:
                content_hash = hash(item['content'][:100])
                if content_hash not in seen_hashes:
                    item['aspect'] = aspect
                    item['matched_query'] = query_text
                    # ã€å…³é”®ã€‘ä¸è¦åœ¨è¿™é‡ŒåŠ  related_geneï¼Œè€Œåœ¨å¤–å±‚åŠ ï¼Œé˜²æ­¢å¤ç”¨é€»è¾‘æ··ä¹±
                    all_results.append(item)
                    seen_hashes.add(content_hash)
        
        return all_results

    def _generate_summary(self, results: List[Dict], subject: str, mode: str) -> str:
        """ç”Ÿæˆ Markdown ç»¼è¿° (æ”¯æŒå¤šåŸºå› åˆ†ç»„)"""
        if not results:
            return f"æœªæ‰¾åˆ°å…³äº {subject} çš„æ–‡çŒ®è¯æ® (æœ¬åœ°+åœ¨çº¿)ã€‚"
            
        lines = []
        lines.append(f"### ğŸ“š æ–‡çŒ®æ£€ç´¢ç»¼è¿°: {subject}")
        lines.append(f"> æ€»è®¡æ¡ç›®: {len(results)} \n")
        
        # å¦‚æœæ˜¯æ‰¹é‡æ¨¡å¼ï¼ŒæŒ‰ related_gene åˆ†ç»„å±•ç¤º
        if mode == "batch_gene":
            # å…ˆæŒ‰ gene æ’åºï¼Œå† groupby
            results.sort(key=lambda x: x.get('related_gene', 'Unknown'))
            for gene, gene_items in groupby(results, key=lambda x: x.get('related_gene', 'Unknown')):
                lines.append(f"#### ğŸ§¬ åŸºå› : {gene}")
                gene_items_list = list(gene_items)
                # å†…éƒ¨å†æŒ‰ aspect åˆ†ç»„
                gene_items_list.sort(key=lambda x: x.get('aspect', 'general'))
                for aspect, group in groupby(gene_items_list, key=lambda x: x.get('aspect', 'general')):
                    aspect_icon = {"clinical": "ğŸ¥", "mechanism": "ğŸ”¬", "therapy": "ğŸ’Š", "general": "ğŸ”"}.get(aspect, "ğŸ“„")
                    lines.append(f"**{aspect_icon} {aspect.capitalize()}**")
                    for item in group:
                        content = item['content'].replace('\n', ' ')[:200] + "..."
                        src = item.get('source_type', 'Unknown')
                        title = item['source_metadata']['paper_title']
                        lines.append(f"- [{src}] {content} *({title})*")
                lines.append("")
        else:
            # å•åŸºå› æˆ– Query æ¨¡å¼
            results.sort(key=lambda x: x.get('aspect', 'general'))
            for aspect, group in groupby(results, key=lambda x: x.get('aspect', 'general')):
                lines.append(f"**{aspect.capitalize()}**")
                for item in group:
                    content = item['content'].replace('\n', ' ')[:250] + "..."
                    lines.append(f"- {content}")
            lines.append("")
        
        return "\n".join(lines)

    def run(self, context: Dict) -> Dict:
        """
        å·¥å…·å…¥å£ - æ”¯æŒæ‰¹é‡ genes å¤„ç† (å¸¦ LLM è‡ªåŠ¨æ€»ç»“å’Œæ–‡ä»¶è½¬å‚¨)
        """
        print(f"    [MongoLocalTool]: æ­£åœ¨æ£€ç´¢æ–‡çŒ®...")
        
        gene = context.get("gene")
        genes = context.get("genes")
        query = context.get("query")
        summaries_list = [] # ğŸ†• ç”¨äºå­˜å‚¨ç²¾ç®€åçš„æ€»ç»“ï¼Œä¼ ç»™ LLM
        all_raw_evidence = [] # ğŸ†• ç”¨äºå­˜å‚¨æ‰€æœ‰åŸå§‹è¯æ®ï¼Œä¿å­˜åˆ°æ–‡ä»¶
        gene_details_map = {}
        search_subject = ""
        
        try:
            # === ä¼˜å…ˆå¤„ç†æ‰¹é‡åŸºå› åˆ—è¡¨ ===
            if genes and isinstance(genes, list) and len(genes) > 0:
                search_mode = "batch_gene"
                search_subject = f"Batch of {len(genes)} genes"
                target_genes = genes 
                
                print(f"  > æ‰¹é‡æ£€ç´¢æ¨¡å¼: {len(target_genes)} ä¸ªåŸºå› ")
                
                for idx, g in enumerate(target_genes):
                    if not g: continue
                    print(f"    [{idx+1}/{len(target_genes)}] æ­£åœ¨æ£€ç´¢å¹¶æ€»ç»“: {g} ...", end="", flush=True)
                    
                    g_res = self._search_evidence_by_gene(g)
                    for item in g_res: item['related_gene'] = g
                    all_raw_evidence.extend(g_res)
                    
                    # ç”Ÿæˆæ€»ç»“
                    summary_text = "æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡çŒ®"
                    if g_res:
                        evidence_text = "\n".join([f"- {r['content'][:300]}..." for r in g_res[:5]])
                        # æç¤ºè¯å¾®è°ƒï¼šè¦æ±‚æ›´ç®€ç»ƒï¼Œæ–¹ä¾¿å±•ç¤º
                        prompt = (
                            f"æ ¹æ®ä»¥ä¸‹åŸºå› ã€{g}ã€‘ä¸è‚ç™Œæ–‡çŒ®ç‰‡æ®µï¼Œç”¨å‡ å¥è¯æ¦‚æ‹¬å…¶ä½œç”¨(æœºåˆ¶/é¢„å/æ²»ç–—)ã€‚"
                            f"ä¸è¦æ¢è¡Œï¼Œ80å­—ä»¥å†…ã€‚\nç‰‡æ®µï¼š\n{evidence_text}"
                        )
                        try:
                            summary_text = summary(prompt).strip()
                            print(" âœ… æ€»ç»“å®Œæˆ")
                        except Exception as e:
                            summary_text = "(æ€»ç»“ç”Ÿæˆå¤±è´¥)"
                            print(f" âš ï¸ æ€»ç»“å¤±è´¥")
                    else:
                        print(" âš ï¸ æ— æ–‡çŒ®")

                    summaries_list.append(f"**{g}**: {summary_text}")
                    
                    # ğŸ†• è®°å½•ç»“æ„åŒ–è¯¦æƒ…
                    gene_details_map[g] = {
                        "count": len(g_res),
                        "summary": summary_text
                    }

            # === å¤„ç†å•ä¸ªåŸºå›  ===
            elif gene:
                # ... (åŒç†å¤„ç†å•åŸºå› ) ...
                search_subject = gene
                g_res = self._search_evidence_by_gene(gene)
                all_raw_evidence.extend(g_res)
                
                evidence_text = "\n".join([f"- {r['content'][:300]}..." for r in g_res[:5]])
                prompt = f"ç”¨ä¸€å¥è¯æ¦‚æ‹¬åŸºå›  {gene} åœ¨è‚ç™Œä¸­çš„ä½œç”¨ã€‚åŸºäºï¼š\n{evidence_text}"
                s_text = summary(prompt).strip()
                summaries_list.append(f"**{gene}**: {s_text}")
                
                gene_details_map[gene] = {
                    "count": len(g_res),
                    "summary": s_text
                }
            
            # ... (Queryæ¨¡å¼ç•¥) ...

            # ä¿å­˜åŸå§‹è¯æ®åˆ°æ–‡ä»¶
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            evidence_filename = f"literature_raw_{timestamp}.json"
            with open(evidence_filename, "w", encoding="utf-8") as f:
                json.dump({
                    "task_genes": genes if genes else [gene],
                    "total_hits": len(all_raw_evidence),
                    "details": all_raw_evidence
                }, f, ensure_ascii=False, indent=2)
            
            final_summary_str = "\n".join(summaries_list)
            
            return {
                "type": "search_literature",
                "subject": search_subject,
                "n_results": len(all_raw_evidence), # æ€»æ•°ä»…ä¾›å‚è€ƒ
                "summary": final_summary_str,
                "gene_details": gene_details_map,   # ğŸ†• å…³é”®ï¼šè¿”å›è¿™ä¸ª map ç»™ Planner
                "raw_results_file": evidence_filename,
                "error": None
            }

        except Exception as e:
            # ... (å¼‚å¸¸å¤„ç†) ...
            return {"type": "search_literature", "error": str(e)}