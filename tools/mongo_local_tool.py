"""MongoLocalTool -> HybridLiteratureTool
æœ¬åœ°å‘é‡æ•°æ®åº“ + PubMed åœ¨çº¿æ··åˆæ£€ç´¢å·¥å…· (é‡æ„ç‰ˆ)

åŠŸèƒ½ï¼š
1. æ··åˆæ£€ç´¢ï¼šåŒæ—¶ä»æœ¬åœ° MongoDB (Vector) å’Œ PubMed Online è·å–è¯æ®
2. å¤šè·¯å¬å›ï¼šGene æ¨¡å¼ä¸‹è‡ªåŠ¨ç”Ÿæˆå¤šç»´æŸ¥è¯¢
3. é²æ£’æ€§ï¼šæœ¬åœ°æˆ–åœ¨çº¿ä»»ä¸€æ¸ é“å¤±è´¥ä¸å½±å“æ•´ä½“è¿è¡Œ

ä¾èµ–: 
- sentence-transformers, faiss-cpu, numpy, pymongo
- PubMedTool (tools.pubmed_tool) <--- æ–°å¢ä¾èµ–
"""

import logging
import re
import time
import numpy as np
import faiss
from typing import List, Dict
from itertools import groupby
from pymongo import MongoClient
from sentence_transformers import SentenceTransformer
from tools.pubmed_tool import PubMedTool

logger = logging.getLogger(__name__)

# === å…¨å±€èµ„æº (å•ä¾‹æ¨¡å¼) ===
_GLOBAL_MODEL = None
_GLOBAL_INDEX = None
_GLOBAL_DOC_MAP = []

class MongoLocalTool:
    
    def __init__(self, host: str = "localhost", port: int = 27017, 
                 db_name: str = "bio", collection_name: str = "evidence_chunks"):
        """
        åˆå§‹åŒ–å·¥å…·
        """
        self.host = host
        self.port = port
        self.db_name = db_name
        self.collection_name = collection_name
        self.client = None
        self.db = None
        self.collection = None
        
        # åˆå§‹åŒ– PubMed åœ¨çº¿å·¥å…·
        # å»ºè®®åœ¨æ­¤å¤„æˆ– executor ä¸­ç»Ÿä¸€ç®¡ç†é‚®ç®±é…ç½®
        self.pubmed = PubMedTool(email="your_email@example.com")

    def _connect(self):
        """è¿æ¥ MongoDB"""
        if self.client: return
        try:
            self.client = MongoClient(host=self.host, port=self.port, serverSelectionTimeoutMS=2000)
            self.db = self.client[self.db_name]
            self.collection = self.db[self.collection_name]
            self.client.admin.command('ping')
            logger.debug(f"Connected to MongoDB {self.host}:{self.port}/{self.db_name}")
        except Exception as e:
            logger.exception(f"Failed to connect to MongoDB: {e}")

    def _ensure_resources(self):
        """åŠ è½½æ¨¡å‹ä¸æ„å»ºç´¢å¼•"""
        global _GLOBAL_MODEL, _GLOBAL_INDEX, _GLOBAL_DOC_MAP
        
        self._connect()
        
        if _GLOBAL_MODEL is None:
            try:
                logger.info(">>> [HybridTool] Loading model (all-MiniLM-L6-v2)...")
                _GLOBAL_MODEL = SentenceTransformer('all-MiniLM-L6-v2')
            except Exception as e:
                logger.error(f"Failed to load SentenceTransformer: {e}")

        if _GLOBAL_INDEX is None and self.collection is not None:
            try:
                logger.info(">>> [HybridTool] Building FAISS index from MongoDB...")
                start_time = time.time()
                # åªè¯»å–å¸¦å‘é‡çš„æ•°æ®
                cursor = self.collection.find(
                    {"vector": {"$exists": True}},
                    {"vector": 1, "text": 1, "section": 1, "paper_title": 1, "source_filename": 1}
                )
                
                vectors = []
                doc_map = []
                for doc in cursor:
                    vec = doc.get('vector')
                    if vec and len(vec) > 0:
                        vectors.append(np.array(vec, dtype='float32'))
                        doc_map.append({
                            'id': str(doc.get('_id')),
                            'text': doc.get('text', ''),
                            'section': doc.get('section', 'Unknown'),
                            'paper_title': doc.get('paper_title', 'Unknown'),
                            'source': doc.get('source_filename', 'LocalDB')
                        })
                
                if vectors:
                    vector_matrix = np.array(vectors)
                    dimension = vector_matrix.shape[1]
                    index = faiss.IndexFlatIP(dimension)
                    index.add(vector_matrix)
                    _GLOBAL_INDEX = index
                    _GLOBAL_DOC_MAP = doc_map
                    logger.info(f">>> Index built with {len(vectors)} chunks in {time.time()-start_time:.2f}s")
                else:
                    logger.warning(">>> No vectors found in local database.")
                    _GLOBAL_INDEX = None
                    _GLOBAL_DOC_MAP = []
            except Exception as e:
                logger.warning(f"Failed to build local index: {e}")

    def _calculate_keyword_score(self, query: str, text: str) -> float:
        if not query or not text: return 0.0
        q_terms = set(re.findall(r'\w+', query.lower()))
        t_terms = set(re.findall(r'\w+', text.lower()))
        if not q_terms: return 0.0
        return len(q_terms.intersection(t_terms)) / len(q_terms)

    # === 1. æœ¬åœ°æ£€ç´¢æ ¸å¿ƒ ===
    def _search_local_core(self, query: str, top_k: int = 5) -> List[Dict]:
        self._ensure_resources()
        if _GLOBAL_INDEX is None or not _GLOBAL_DOC_MAP:
            return []

        try:
            query_vector = _GLOBAL_MODEL.encode([query])
            query_vector = np.array(query_vector, dtype='float32')
            D, I = _GLOBAL_INDEX.search(query_vector, min(50, len(_GLOBAL_DOC_MAP)))
            
            candidates = []
            for rank, idx in enumerate(I[0]):
                if idx == -1: continue
                doc_data = _GLOBAL_DOC_MAP[idx]
                vec_score = float(D[0][rank])
                kw_score = self._calculate_keyword_score(query, doc_data['text'])
                
                # æ··åˆæ‰“åˆ†
                hybrid_score = (0.7 * vec_score) + (0.3 * kw_score)
                
                # ç« èŠ‚åŠ æƒ
                section = str(doc_data['section']).lower()
                multiplier = 1.0
                if any(x in section for x in ['result', 'discussion', 'conclusion']):
                    multiplier = 1.2
                elif 'abstract' in section:
                    multiplier = 1.1
                
                candidates.append({
                    "content": doc_data['text'],
                    "source_metadata": {
                        "paper_title": doc_data['paper_title'],
                        "section": doc_data['section'],
                        "filename": doc_data['source']
                    },
                    "scores": {"final": round(hybrid_score * multiplier, 4)},
                    "source_type": "Local" # æ ‡è®°æ¥æº
                })
            
            candidates.sort(key=lambda x: x['scores']['final'], reverse=True)
            return candidates[:top_k]
        except Exception as e:
            logger.error(f"Local search failed: {e}")
            return []

    # === 2. æ··åˆæ‰§è¡Œé€»è¾‘ (Local + PubMed) ===
    def _hybrid_search(self, query: str, top_k_local: int = 5, top_k_online: int = 5) -> List[Dict]:
        """åˆå¹¶æœ¬åœ°å’Œåœ¨çº¿ç»“æœ"""
        # 1. æœ¬åœ°æ£€ç´¢
        local_res = self._search_local_core(query, top_k=top_k_local)
        
        # 2. åœ¨çº¿æ£€ç´¢ (è°ƒç”¨å¤–éƒ¨å·¥å…·)
        online_res = self.pubmed.search(query, max_results=top_k_online)
        
        # 3. åˆå¹¶
        combined = local_res + online_res
        return combined

    def _search_evidence_by_gene(self, gene_name: str) -> List[Dict]:
        """é’ˆå¯¹ Gene çš„å¤šè·¯æ··åˆå¬å›"""
        queries = [
            ("clinical_prognosis", f"{gene_name} hepatocellular carcinoma prognosis survival"),
            ("mechanism", f"{gene_name} signaling pathway liver cancer mechanism"),
            ("drug_therapy", f"{gene_name} inhibitor therapeutic target HCC")
        ]
        
        all_results = []
        seen_hashes = set()
        
        for aspect, query_text in queries:
            # æ··åˆæ£€ç´¢ï¼šæœ¬åœ° 3 æ¡ + åœ¨çº¿ 2 æ¡
            results = self._hybrid_search(query_text, top_k_local=3, top_k_online=2)
            
            for item in results:
                # å»é‡
                content_hash = hash(item['content'][:100])
                if content_hash not in seen_hashes:
                    item['aspect'] = aspect
                    item['matched_query'] = query_text
                    all_results.append(item)
                    seen_hashes.add(content_hash)
        
        return all_results

    def _generate_summary(self, results: List[Dict], subject: str) -> str:
        """ç”Ÿæˆ Markdown ç»¼è¿°"""
        if not results:
            return f"æœªæ‰¾åˆ°å…³äº {subject} çš„æ–‡çŒ®è¯æ® (æœ¬åœ°+åœ¨çº¿)ã€‚"
            
        results.sort(key=lambda x: x.get('aspect', 'general'))
        
        lines = []
        lines.append(f"### ğŸ“š {subject} æ–‡çŒ®è¯æ®ç»¼è¿° (æ··åˆæ£€ç´¢)")
        local_count = sum(1 for r in results if r.get('source_type') == 'Local')
        online_count = sum(1 for r in results if r.get('source_type') == 'Online')
        lines.append(f"> æ£€ç´¢ç»“æœ: {len(results)} æ¡ (æœ¬åœ°: {local_count}, åœ¨çº¿ PubMed: {online_count})\n")
        
        for aspect, group in groupby(results, key=lambda x: x.get('aspect', 'general')):
            title_map = {
                "clinical_prognosis": "ğŸ¥ ä¸´åºŠé¢„å (Prognosis)",
                "mechanism": "ğŸ”¬ åˆ†å­æœºåˆ¶ (Mechanism)",
                "drug_therapy": "ğŸ’Š è¯ç‰©æ²»ç–— (Therapy)",
                "general": "ğŸ” é€šç”¨æ£€ç´¢ç»“æœ"
            }
            display_title = title_map.get(aspect, aspect.capitalize())
            lines.append(f"**{display_title}**")
            
            for item in group:
                content = item['content'].replace('\n', ' ')
                if len(content) > 300: content = content[:300] + "..."
                
                title = item['source_metadata']['paper_title']
                src_type = item.get('source_type', 'Local')
                icon = "ğŸ " if src_type == "Local" else "ğŸŒ"
                
                lines.append(f"- {icon} [{src_type}] {content} *[Src: {title}]*")
            lines.append("")
        
        return "\n".join(lines)

    def run(self, context: Dict) -> Dict:
        """
        å·¥å…·å…¥å£
        """
        print(f"[MongoLocalTool]: æ­£åœ¨æ£€ç´¢æ–‡çŒ®\n")
        gene = context.get("gene")
        if not gene and context.get("genes"): gene = context.get("genes")[0]
        query = context.get("query")
        
        results = []
        search_subject = ""
        search_mode = ""

        try:
            if gene:
                search_subject = gene
                search_mode = "gene_hybrid_mining"
                logger.info(f"Running Gene Hybrid Mode for: {gene}")
                results = self._search_evidence_by_gene(gene)
                
            elif query:
                search_subject = query
                search_mode = "general_hybrid_search"
                logger.info(f"Running General Hybrid Mode for: {query}")
                results = self._hybrid_search(query, top_k_local=3, top_k_online=3)
                for r in results: r['aspect'] = 'general'
            
            else:
                return {"type": "search_literature", "error": "No gene/query provided"}

            summary = self._generate_summary(results, search_subject)

            return {
                "type": "search_literature",
                "subject": search_subject,
                "search_mode": search_mode,
                "n_results": len(results),
                "summary": summary,
                "raw_results": results,
                "error": None
            }

        except Exception as e:
            logger.exception(f"Error in Hybrid Tool: {e}")
            return {
                "type": "search_literature",
                "error": str(e),
                "summary": f"æ£€ç´¢å‡ºé”™: {str(e)}",
                "results": []
            }

# === éªŒè¯ ===
if __name__ == "__main__":
    import sys
    logging.basicConfig(level=logging.INFO)
    print("ğŸš€ Testing Hybrid Tool (Local + PubMed)...")
    
    try:
        # è¯·ç¡®ä¿ MongoDB æœ‰ evidence_chunks é›†åˆï¼Œæˆ–è€…å®ƒä¼šä¼˜é›…é™çº§åªæ˜¾ç¤º Online ç»“æœ
        tool = MongoLocalTool(db_name="bio", collection_name="evidence_chunks")
        res = tool.run({"gene": "TP53"})
        print(f"\n{res['summary']}")
    except Exception as e:
        print(f"Failed: {e}")