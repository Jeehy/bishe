"""MongoLocalTool - æœ¬åœ°æ–‡çŒ®å‘é‡æ£€ç´¢å·¥å…· (å¢å¼ºç‰ˆ)

åŠŸèƒ½ï¼š
1. æ··åˆæ£€ç´¢ï¼šåŸºäº FAISS çš„å‘é‡æ£€ç´¢ + å…³é”®è¯åŒ¹é…
2. å¤šè·¯å¬å›ï¼šé’ˆå¯¹ Gene è‡ªåŠ¨ç”Ÿæˆå¤šç»´åº¦æŸ¥è¯¢ (Prognosis/Mechanism/Drug)
3. è¯æ®åˆæˆï¼šè‡ªåŠ¨ç”Ÿæˆ Markdown æ ¼å¼çš„è¯æ®ç»¼è¿°

æ•°æ®çº¦å®šï¼š
- MongoDB: localhost:27017, db=bio, collection=evidence_chunks
- ä¾èµ–: sentence-transformers, faiss-cpu, numpy
"""

import logging
import re
import time
import numpy as np
import faiss
from typing import List, Dict
from itertools import groupby
from pymongo import MongoClient
from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)

# === å…¨å±€èµ„æº (å•ä¾‹æ¨¡å¼) ===
_GLOBAL_MODEL = None
_GLOBAL_INDEX = None
_GLOBAL_DOC_MAP = []

class MongoLocalTool:
    
    def __init__(self, host: str = "localhost", port: int = 27017, 
                 db_name: str = "bio", collection_name: str = "evidence_chunks"):
        """
        åˆå§‹åŒ–é…ç½®
        """
        self.host = host
        self.port = port
        self.db_name = db_name
        self.collection_name = collection_name
        self.client = None
        self.db = None
        self.collection = None

    def _connect(self):
        """è¿æ¥æ•°æ®åº“"""
        if self.client: return
        try:
            self.client = MongoClient(host=self.host, port=self.port, serverSelectionTimeoutMS=2000)
            self.db = self.client[self.db_name]
            self.collection = self.db[self.collection_name]
        except Exception as e:
            logger.exception(f"Failed to connect to MongoDB: {e}")
            raise

    def _ensure_resources(self):
        """
        åŠ è½½æ¨¡å‹ä¸æ„å»ºç´¢å¼•ã€‚
        è€—æ—¶æ“ä½œï¼Œä»…åœ¨é¦–æ¬¡è°ƒç”¨æ—¶æ‰§è¡Œã€‚
        """
        global _GLOBAL_MODEL, _GLOBAL_INDEX, _GLOBAL_DOC_MAP
        self._connect()
        
        # 1. åŠ è½½æ¨¡å‹
        if _GLOBAL_MODEL is None:
            logger.info(">>> [MongoLocalTool] Loading model (all-MiniLM-L6-v2)...")
            try:
                _GLOBAL_MODEL = SentenceTransformer('all-MiniLM-L6-v2')
            except Exception as e:
                logger.error(f"Failed to load SentenceTransformer: {e}")
                raise

        # 2. æ„å»ºç´¢å¼• (ä» MongoDB è¯»å–å‘é‡)
        if _GLOBAL_INDEX is None:
            logger.info(">>> [MongoLocalTool] Building FAISS index from MongoDB...")
            start_time = time.time()
            cursor = self.collection.find(
                {"vector": {"$exists": True}},
                {"vector": 1, "text": 1, "section": 1, "paper_title": 1, "source_filename": 1}
            )
            vectors = []
            doc_map = []
            for doc in cursor:
                vec = doc.get('vector')
                if vec and len(vec) > 0:
                    vectors.append(np.array(vec, dtype='float32'))
                    doc_map.append({
                        'id': str(doc.get('_id')),
                        'text': doc.get('text', ''),
                        'section': doc.get('section', 'Unknown'),
                        'paper_title': doc.get('paper_title', 'Unknown'),
                        'source': doc.get('source_filename', '')
                    })
            
            if vectors:
                vector_matrix = np.array(vectors)
                dimension = vector_matrix.shape[1]
                # ä½¿ç”¨å†…ç§¯ (Inner Product) ç´¢å¼•ï¼Œå‡è®¾å‘é‡å·²å½’ä¸€åŒ–åˆ™ç­‰åŒäºä½™å¼¦ç›¸ä¼¼åº¦
                index = faiss.IndexFlatIP(dimension)
                index.add(vector_matrix)
                _GLOBAL_INDEX = index
                _GLOBAL_DOC_MAP = doc_map
                logger.info(f">>> [MongoLocalTool] Index built with {len(vectors)} chunks in {time.time()-start_time:.2f}s")
            else:
                logger.warning(">>> [MongoLocalTool] No vectors found in database! Search will return empty.")
                _GLOBAL_INDEX = None
                _GLOBAL_DOC_MAP = []

    def _calculate_keyword_score(self, query: str, text: str) -> float:
        """å…³é”®è¯è¦†ç›–ç‡æ‰“åˆ†"""
        if not query or not text: return 0.0
        q_terms = set(re.findall(r'\w+', query.lower()))
        t_terms = set(re.findall(r'\w+', text.lower()))
        if not q_terms: return 0.0
        return len(q_terms.intersection(t_terms)) / len(q_terms)

    def _search_core(self, query: str, top_k: int = 8, alpha: float = 0.7, fetch_k: int = 50) -> List[Dict]:
        """
        æ ¸å¿ƒæ£€ç´¢æ–¹æ³•ï¼šFAISS Vector + å…³é”®è¯æ··åˆæ‰“åˆ†
        """
        self._ensure_resources()
        if _GLOBAL_INDEX is None or not _GLOBAL_DOC_MAP:
            return []

        # 1. å‘é‡æ£€ç´¢ (Vector Search)
        query_vector = _GLOBAL_MODEL.encode([query])
        query_vector = np.array(query_vector, dtype='float32')
        # æ£€ç´¢ top fetch_k ä¸ªå‘é‡å€™é€‰
        D, I = _GLOBAL_INDEX.search(query_vector, min(fetch_k, len(_GLOBAL_DOC_MAP)))
        
        candidates = []
        for rank, idx in enumerate(I[0]):
            if idx == -1: continue
            doc_data = _GLOBAL_DOC_MAP[idx]
            # å‘é‡åˆ†æ•°
            vec_score = float(D[0][rank])
            # å…³é”®è¯åˆ†æ•°
            kw_score = self._calculate_keyword_score(query, doc_data['text'])
            # 2. æ··åˆæ‰“åˆ† (Hybrid Scoring)
            # alpha æ§åˆ¶å‘é‡æ£€ç´¢æƒé‡çš„å æ¯”
            hybrid_score = (alpha * vec_score) + ((1 - alpha) * kw_score)
            
            # 3. ç« èŠ‚åŠ æƒ (Section Boosting)
            # ä¼˜å…ˆå±•ç¤ºç»“æœä¸è®¨è®ºéƒ¨åˆ†
            section = str(doc_data['section']).lower()
            multiplier = 1.0
            if any(x in section for x in ['result', 'discussion', 'conclusion']):
                multiplier = 1.2
            elif 'abstract' in section:
                multiplier = 1.1
            final_score = hybrid_score * multiplier
            candidates.append({
                "content": doc_data['text'],
                "source_metadata": {
                    "paper_title": doc_data['paper_title'],
                    "section": doc_data['section'],
                    "filename": doc_data['source']
                },
                "scores": {
                    "final": round(final_score, 4),
                    "vector": round(vec_score, 4),
                    "keyword": round(kw_score, 4)
                }
            })

        # 4. é‡æ–°æ’åº
        candidates.sort(key=lambda x: x['scores']['final'], reverse=True)
        return candidates[:top_k]

    def _search_evidence_by_gene(self, gene_name: str) -> List[Dict]:
        """
        é’ˆå¯¹ç‰¹å®šåŸºå› çš„å¤šè·¯å¬å›ç­–ç•¥
        ç”Ÿæˆ 3 ä¸ªä¸åŒä¾§é‡ç‚¹çš„ Queryï¼Œåˆ†åˆ«è°ƒç”¨æ£€ç´¢
        """
        queries = [
            ("clinical_prognosis", f"{gene_name} high expression prognosis survival rate HCC"),
            ("mechanism", f"{gene_name} signaling pathway mechanism proliferation invasion liver cancer"),
            ("drug_therapy", f"{gene_name} inhibitor therapeutic target drug efficacy HCC")
        ]
        
        all_results = []
        seen_contents = set()
        
        for aspect, query_text in queries:
            # åˆ†åˆ«æ£€ç´¢ï¼Œæ¯ä¸ªç»´åº¦å– Top 3
            results = self._search_core(query_text, top_k=5, alpha=0.7, fetch_k=30)
            
            for item in results:
                # ç®€å•å»é‡ (å–å‰50å­—ç¬¦å“ˆå¸Œ)
                signature = item['content'][:50]
                if signature not in seen_contents:
                    item['aspect'] = aspect
                    item['matched_query'] = query_text
                    all_results.append(item)
                    seen_contents.add(signature)
        
        return all_results

    def _generate_summary(self, results: List[Dict], subject: str) -> str:
        """
        ç”Ÿæˆ Markdown æ ¼å¼çš„è¯æ®ç»¼è¿°
        """
        if not results:
            return f"æœªæ‰¾åˆ°å…³äº {subject} çš„æ–‡çŒ®è¯æ®ã€‚"
            
        # åˆ†ç»„ (general / clinical_prognosis / mechanism / drug_therapy)
        results.sort(key=lambda x: x.get('aspect', 'general'))
        lines = []
        lines.append(f"### ğŸ“š {subject} æ–‡çŒ®è¯æ®ç»¼è¿°")
        lines.append(f"> å…±æ£€ç´¢åˆ° {len(results)} æ¡ç›¸å…³è¯æ®ç‰‡æ®µã€‚\n")
        
        for aspect, group in groupby(results, key=lambda x: x.get('aspect', 'general')):
            title_map = {
                "clinical_prognosis": "ğŸ¥ ä¸´åºŠé¢„å (Prognosis)",
                "mechanism": "ğŸ”¬ åˆ†å­æœºåˆ¶ (Mechanism)",
                "drug_therapy": "ğŸ’Š è¯ç‰©æ²»ç–— (Therapy)",
                "general": "ğŸ” é€šç”¨æ£€ç´¢ç»“æœ"
            }
            display_title = title_map.get(aspect, aspect.capitalize())
            lines.append(f"**{display_title}**")
            
            for item in group:
                content = item['content'].replace('\n', ' ')
                # æˆªæ–­è¿‡é•¿æ–‡æœ¬
                if len(content) > 300:
                    content = content[:300] + "..."
                source = item['source_metadata']['paper_title']
                score = item['scores']['final']
                lines.append(f"- {content} *[Score: {score:.2f} | Src: {source}]*")
            lines.append("") 
        
        return "\n".join(lines)

    def run(self, context: Dict) -> Dict:
        """
        å·¥å…·ç»Ÿä¸€å…¥å£
        """
        # 1. å‚æ•°è§£æ
        gene = context.get("gene")
        if not gene and context.get("genes"):
            gene = context.get("genes")[0]
        query = context.get("query")
        results = []
        search_subject = ""
        search_mode = ""

        try:
            if gene:
                # === è·¯å¾„ A: åŸºå› å¤šè·¯å¬å›æ¨¡å¼ ===
                search_subject = gene
                search_mode = "gene_evidence_mining"
                logger.info(f"Running Gene Mode for: {gene}")
                results = self._search_evidence_by_gene(gene)
                
            elif query:
                # === è·¯å¾„ B: é€šç”¨æŸ¥è¯¢æ··åˆæ£€ç´¢æ¨¡å¼ ===
                search_subject = query
                search_mode = "general_hybrid_search"
                logger.info(f"Running Query Mode for: {query}")
                results = self._search_core(query, top_k=5)
                # æ ‡è®° aspect ä»¥ä¾¿ç”Ÿæˆ summary
                for r in results:
                    r['aspect'] = 'general'
            
            else:
                return {
                    "type": "search_literature",
                    "error": "No 'gene' or 'query' provided."
                }

            # 3. ç”Ÿæˆ Markdown ç»¼è¿°
            summary = self._generate_summary(results, search_subject)

            # 4. è¿”å›ç»“æœ
            return {
                "type": "search_literature",
                "subject": search_subject,
                "search_mode": search_mode,
                "n_results": len(results),
                "summary": summary,       # <--- LLM æ ¸å¿ƒé˜…è¯»å†…å®¹
                "raw_results": results,   # <--- ä¿ç•™åŸå§‹æ•°æ®ç»“æ„
                "error": None
            }

        except Exception as e:
            logger.exception(f"Error in MongoLocalTool run: {e}")
            return {
                "type": "search_literature",
                "error": str(e),
                "summary": f"æ£€ç´¢å‡ºé”™: {str(e)}",
                "results": []
            }


# =============================================================================
# éªŒè¯ç”¨çš„ Main å‡½æ•° (Self-Check)
# =============================================================================
if __name__ == "__main__":
    import sys
    
    # é…ç½®æ§åˆ¶å°æ—¥å¿—ï¼Œæ–¹ä¾¿è§‚å¯Ÿå†…éƒ¨æµç¨‹
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

    print("\nğŸš€ [MongoLocalTool] å¯åŠ¨è‡ªæ£€ç¨‹åº (Vector Ready Mode)...\n")
    
    # 1. åˆå§‹åŒ–å·¥å…·
    # ç¡®ä¿ MongoDB æœåŠ¡å·²å¼€å¯ï¼Œä¸” bio.evidence_chunks é›†åˆä¸­æœ‰å¸¦ vector å­—æ®µçš„æ•°æ®
    try:
        print("Creating tool instance...")
        tool = MongoLocalTool(db_name="bio", collection_name="evidence_chunks")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        sys.exit(1)

    # ---------------------------------------------------------
    # æµ‹è¯•åœºæ™¯ 1: Gene æ¨¡å¼ (æ ¸å¿ƒåŠŸèƒ½)
    # é¢„æœŸ: è‡ªåŠ¨ç”Ÿæˆå¤šæ¡æŸ¥è¯¢ï¼Œæ‰§è¡Œå¤šæ¬¡æ£€ç´¢ï¼Œæœ€ååˆå¹¶ç”Ÿæˆç»¼è¿°
    # ---------------------------------------------------------
    test_gene = "TP53"
    print(f"\n" + "="*60)
    print(f"ğŸ§ª [Test 1] Testing Gene Mode for '{test_gene}'")
    print("é¢„æœŸè¡Œä¸º: è§¦å‘ gene_evidence_mining æ¨¡å¼ï¼Œè¿›è¡Œå¤šè·¯å¬å›")
    print("="*60)
    
    start_time = time.time()
    res1 = tool.run({"gene": test_gene})
    duration = time.time() - start_time
    
    if res1.get("error"):
        print(f"âŒ Error: {res1['error']}")
    else:
        print(f"âœ… Success! (è€—æ—¶ {duration:.2f}s)")
        print(f"   - Search Mode: {res1.get('search_mode')} (åº”ä¸º gene_evidence_mining)")
        print(f"   - Total Results: {res1.get('n_results')}")
        
        # æ£€æŸ¥æ˜¯å¦çœŸçš„ç”¨åˆ°äº†å‘é‡æ£€ç´¢ (æ£€æŸ¥ç¬¬ä¸€æ¡ç»“æœæ˜¯å¦æœ‰ scores.vector)
        if res1.get('raw_results') and 'vector' in res1['raw_results'][0]['scores']:
            vec_score = res1['raw_results'][0]['scores']['vector']
            print(f"   - Vector Score Example: {vec_score:.4f} (è¯æ˜ä½¿ç”¨äº†å‘é‡æ£€ç´¢)")
        else:
            print("   âš ï¸ Warning: æœªæ£€æµ‹åˆ°å‘é‡åˆ†æ•°ï¼Œè¯·æ£€æŸ¥æ•°æ®åº“ vector å­—æ®µ")

        print("\nğŸ“ [Summary Preview]:")
        print("-" * 40)
        # æ‰“å°æ‘˜è¦çš„å‰ 500 ä¸ªå­—ç¬¦
        print(res1.get('summary', '')[:500].replace('\n', ' ') + "...") 
        print("-" * 40)

    # ---------------------------------------------------------
    # æµ‹è¯•åœºæ™¯ 2: Query æ¨¡å¼ (é€šç”¨æ£€ç´¢)
    # é¢„æœŸ: å¯¹è¾“å…¥è¯­å¥è¿›è¡Œå•æ¬¡æ··åˆæ£€ç´¢
    # ---------------------------------------------------------
    test_query = "liver cancer immunotherapy efficacy"
    print(f"\n" + "="*60)
    print(f"ğŸ§ª [Test 2] Testing Query Mode for '{test_query}'")
    print("é¢„æœŸè¡Œä¸º: è§¦å‘ general_hybrid_search æ¨¡å¼")
    print("="*60)
    
    res2 = tool.run({"query": test_query})
    
    if res2.get("error"):
        print(f"âŒ Error: {res2['error']}")
    else:
        print(f"âœ… Success!")
        print(f"   - Search Mode: {res2.get('search_mode')} (åº”ä¸º general_hybrid_search)")
        print(f"   - Total Results: {res2.get('n_results')}")
        
        print("\nğŸ“ [Summary Preview]:")
        print("-" * 40)
        print(res2.get('summary', '')[:300].replace('\n', ' ') + "...")
        print("-" * 40)

    print("\nğŸ è‡ªæ£€å®Œæˆã€‚")